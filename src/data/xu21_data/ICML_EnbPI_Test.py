from ICML_EnbPI_Class import prediction_interval
import ICML_EnbPI_helpers as alg_slide_k
from ICML_EnbPI_helpers import plot_average_new, grouped_box_new, one_dimen_transform, network_gen_data, network_transform
from keras.layers import LSTM
from scipy.stats import beta
from matplotlib.lines import Line2D  # For legend handles
import statsmodels as sm
import calendar
import warnings
import matplotlib.pyplot as plt
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor
import itertools
import importlib
import time
import pandas as pd
import numpy as np
import os
import sys
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.optimizers import Adam
warnings.filterwarnings("ignore")

'''This File contains code for reproducing all figures except Figure 4, which is generated by another code (ICML_ECAD_test.py)'''

'''Read Data'''


def read_data(i, filename, max_data_size):
    if i == 0:
        '''
            All datasets are Multivariate time-series. They have respective Github for more details as well.
            1. Greenhouse Gas Observing Network Data Set
            Time from 5.10-7.31, 2010, with 4 samples everyday, 6 hours apart between data poits.
            Goal is to "use inverse methods to determine the optimal values of the weights in the weighted sum of 15 tracers that best matches the synthetic observations"
            In other words, find weights so that first 15 tracers will be as close to the last as possible.
            Note, data at many other grid cells are available. Others are in Downloads/ðŸŒŸAISTATS Data/Greenhouse Data
            https://archive.ics.uci.edu/ml/datasets/Greenhouse+Gas+Observing+Network
        '''
        data = pd.read_csv(filename, header=None, sep=' ').T
        # data.shape  # 327, 16Note, rows are 16 time series (first 15 from tracers, last from synthetic).
    elif i == 1:
        '''
            2. Appliances energy prediction Data Set
            The data set is at 10 min for about 4.5 months.
            The column named 'Appliances' is the response. Other columns are predictors
            https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction
        '''
        data = pd.read_csv(filename, delimiter=',')
        # data.shape  # (19736, 29)
        data.drop('date', inplace=True, axis=1)
        data.loc[:, data.columns != 'Appliances']
    elif i == 2:
        '''
            3. Beijing Multi-Site Air-Quality Data Data Set
            This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites.
            Time period from 3.1, 2013 to 2.28, 2017.
            PM2.5 or PM10 would be the response.
            https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data
        '''
        data = pd.read_csv(filename)
        # data.shape  # 35064, 18
        # data.columns
        data.drop(columns=['No', 'year', 'month', 'day', 'hour',
                           'wd', 'station'], inplace=True, axis=1)
        data.dropna(inplace=True)
        # data.shape  # 32907, 11
        # data.head(5)
    else:
        """
            4 (Alternative). NREL Solar data at Atlanta Downtown in 2018. 24 observations per day and separately equally by 1H @ half an hour mark everytime
            Data descriptions see Solar Writeup
            Data download:
            (With API) https://nsrdb.nrel.gov/data-sets/api-instructions.html
            (Manual) https://maps.nrel.gov/nsrdb-viewer
        """
        data = pd.read_csv(filename, skiprows=2)
        # data.shape  # 8760, 14
        data.drop(columns=data.columns[0:5], inplace=True)
        data.drop(columns='Unnamed: 13', inplace=True)
        # data.shape  # 8760, 8
        # data.head(5)
    # pick maximum of X data points (for speed)
    data = data.iloc[:min(max_data_size, data.shape[0]), :]
    print(data.shape)
    return data


def read_CA_data(filename):
    data = pd.read_csv(filename)
    # data.shape  # 8760, 14
    data.drop(columns=data.columns[0:6], inplace=True)
    return data


def read_wind_data():
    ''' Note, just use the 8760 hourly observation in 2019
    Github repo is here: https://github.com/Duvey314/austin-green-energy-predictor'''
    data_wind_19 = pd.read_csv('Wind_Hackberry_Generation_2019_2020.csv')
    data_wind_19 = data_wind_19.iloc[:24*365, :]
    return data_wind_19


'''Appendix: Data Visualization'''
dataSolar_Atl = read_data(3, 'Solar_Atl_data.csv', max_data_size)
dataPalo_Atlo = read_CA_data('Palo_Alto_data.csv')
dataWind_Austin = read_wind_data()
# Visualization
font = {'size': 16}
plt.rc('font', **font)
fig, ax = plt.subplots(3, 1, figsize=(12, 6), sharex=True)
length = 2000
period = range(length)
ax[0].plot(period, dataSolar_Atl['DHI'][:length], color='blue')
ax[0].set_title('Atlanta Solar Radiation')
ax[0].set_ylabel('DHI')
ax[1].plot(period, dataPalo_Alto['DHI'][:length], color='blue')
ax[1].set_title('Palo Alto Solar Radiation')
ax[1].set_ylabel('DHI')
ax[2].plot(period, dataWind_Austin['MWH'][:length], color='blue')
ax[2].set_title('Austin Wind Energy')
ax[2].set_ylabel('MWH')
ax[2].set_xlabel('Time Index')
fig.savefig('Raw_data_plot.pdf', dpi=300, bbox_inches='tight',
            pad_inches=0)

'''   Real-Data Experiments
    Stride s=1 for All four datasets'''
# Read Data:
data0 = read_data(0, 'green_house_data.csv', max_data_size)
data1 = read_data(1, 'appliances_data.csv', max_data_size)
data2 = read_data(
    2, 'Beijing_air_Tiantan_data.csv', max_data_size)
CA_cities = ['Fremont', 'Milpitas', 'Mountain_View', 'North_San_Jose',
             'Palo_Alto', 'Redwood_City', 'San_Mateo', 'Santa_Clara',
             'Sunnyvale']
for city in CA_cities:
    globals()['data%s' % city] = read_CA_data(f'{city}_data.csv')

# Initialize Parameters
max_data_size = 10000


def big_transform(CA_cities, current_city, one_dim, train_size):
    # Next, merge these data (so concatenate X_t and Y_t for one_d or not)
    # Return [X_train, X_test, Y_train, Y_test] from data_x and data_y
    # Data_x is either multivariate (direct concatenation)
    # or univariate (transform each series and THEN concatenate the transformed series)
    big_X_train = []
    big_X_predict = []
    for city in CA_cities:
        data = eval(f'data{city}')  # Pandas DataFrame
        data_x = data.loc[:, data.columns != 'DHI']
        data_y = data['DHI']
        data_x_numpy = data_x.to_numpy()  # Convert to numpy
        data_y_numpy = data_y.to_numpy()  # Convert to numpy
        X_train = data_x_numpy[:train_size, :]
        X_predict = data_x_numpy[train_size:, :]
        Y_train_del = data_y_numpy[:train_size]
        Y_predict_del = data_y_numpy[train_size:]
        if city == current_city:
            Y_train = Y_train_del
            Y_predict = Y_predict_del
        if one_dim:
            X_train, X_predict, Y_train_del, Y_predict_del = one_dimen_transform(
                Y_train_del, Y_predict_del, d=20)
            big_X_train.append(X_train)
            big_X_predict.append(X_predict)
            if city == current_city:
                Y_train = Y_train_del
        else:
            big_X_train.append(X_train)
            big_X_predict.append(X_predict)
    X_train = np.hstack(big_X_train)
    X_predict = np.hstack(big_X_predict)
    return([X_train, X_predict, Y_train, Y_predict])


'''Part 1, for 1-\alpha plot and boxplots, s=1 '''

stride = 1
miss_test_idx = []
alpha = 0.1
tot_trial = 10  # For CP methods that randomizes
np.random.seed(98765)
B = 30  # number of bootstrap samples
Data_name = ['green_house', 'appliances', 'Beijing_air',
             'Solar_Atl', 'Palo_Alto', 'Wind_Austin']
Data_name_network = ['Palo_Alto']
response_ls = {'green_house': 15, 'appliances': 'Appliances',
               'Beijing_air': 'PM2.5', 'Solar_Atl': 'DHI', 'Wind_Austin': 'MWH'}
response_ls_network = {'Palo_Alto': 'DHI'}
data_ind = {}
for i in range(len(Data_name)):
    key = Data_name[i]
    if i <= 2:
        data_ind[key] = i
    else:
        data_ind[key] = key
data_ind_network = {'Palo_Alto': 'Palo_Alto'}
min_alpha = 0.0001
max_alpha = 10
ridge_cv = RidgeCV(alphas=np.linspace(min_alpha, max_alpha, 10))
random_forest = RandomForestRegressor(n_estimators=10, criterion='mse',
                                      bootstrap=False, max_depth=2, n_jobs=-1)


def keras_mod():
    model = Sequential(name='NeuralNet')
    model.add(Dense(100, activation='relu'))
    model.add(Dense(100, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(100, activation='relu'))
    model.add(Dense(1, activation='relu'))
    # Compile model
    opt = Adam(0.0005)
    model.compile(loss='mean_squared_error', optimizer=opt)
    return model


def keras_rnn():
    model = Sequential(name='RNN')
    # For fast cuDNN implementation, activation = 'relu' does not work
    model.add(LSTM(100, activation='tanh', return_sequences=True))
    model.add(LSTM(100, activation='tanh'))
    model.add(Dense(1, activation='relu'))
    # Compile model
    opt = Adam(0.0005)
    model.compile(loss='mean_squared_error', optimizer=opt)
    return model


''' Get average coverage and width for different train_size
    Use Ridge, RF, NN, and RNN
    Note, ARIMA is NOT ran here'''
tot_trial = 10
rnn = False
energy_data = True  # True for Palo Alto only, as it is a network. So need to run the procedure TWICE
if energy_data:
    Data_name = Data_name
    response_ls = response_ls
    data_ind = data_ind
else:
    Data_name = Data_name_network
    response_ls = response_ls_network
    data_ind = data_ind_network
for one_dim in [True, False]:
    methods = ['Ensemble', 'ICP', 'Weighted_ICP']
    # Run Ridge, Lasso, RF, and NN
    for data_name in Data_name:
        data = eval(f'data{data_ind[data_name]}')  # Pandas DataFrame
        data_x = data.loc[:, data.columns != response_ls[data_name]]
        data_y = data[response_ls[data_name]]
        data_x_numpy = data_x.to_numpy()  # Convert to numpy
        data_y_numpy = data_y.to_numpy()  # Convert to numpy
        total_data_points = data_x_numpy.shape[0]
        Train_size = np.linspace(0.1 * total_data_points,
                                 0.3 * total_data_points, 10).astype(int)
        Train_size = [Train_size[0], Train_size[4], Train_size[8]]
        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',
                                        'method', 'train_size', 'coverage', 'width'])
        for itrial in range(tot_trial):
            np.random.seed(98765 + itrial)
            for train_size in Train_size:
                nnet = keras_mod()  # Note, this is necessary because a model may "remember the past"
                rnnet = keras_rnn()
                print(f'At trial # {itrial} and train_size={train_size}')
                print(f'For {data_name}')
                if energy_data:
                    X_train, X_predict, Y_train, Y_predict = big_transform(
                        Data_name, data_name, one_dim, train_size)
                else:
                    X_train = data_x_numpy[:train_size, :]
                    X_predict = data_x_numpy[train_size:, :]
                    Y_train = data_y_numpy[:train_size]
                    Y_predict = data_y_numpy[train_size:]
                    if one_dim:
                        X_train, X_predict, Y_train, Y_predict = one_dimen_transform(
                            Y_train, Y_predict, d=20)
                ridge_results = prediction_interval(
                    ridge_cv, X_train, X_predict, Y_train, Y_predict)
                rf_results = prediction_interval(
                    random_forest,  X_train, X_predict, Y_train, Y_predict)
                nn_results = prediction_interval(
                    nnet,  X_train, X_predict, Y_train, Y_predict)
                if rnn:
                    T, k = X_train.shape
                    T1 = X_predict.shape[0]
                    X_train = X_train.reshape((T, 1, k))
                    X_predict = X_predict.reshape((T1, 1, k))
                    rnn_results = prediction_interval(
                        rnnet, X_train, X_predict, Y_train, Y_predict)
                # For CP Methods
                print(f'regressor is {ridge_cv.__class__.__name__}')
                result_ridge = ridge_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                print(f'regressor is {random_forest.__class__.__name__}')
                result_rf = rf_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                print(f'regressor is {nnet.name}')
                result_nn = nn_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                result_nn['muh_fun'] = 'NeuralNet'
                if rnn:
                    print(f'regressor is {rnnet.name}')
                    result_rnn = rnn_results.run_experiments(
                        alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                    result_rnn['muh_fun'] = 'RNN'
                    results = pd.concat([results, result_ridge, result_rf, result_nn, result_rnn])
                else:
                    results = pd.concat([results, result_ridge, result_rf, result_nn])
                if one_dim:
                    results.to_csv(
                        f'{data_name}_many_train_new_1d.csv', index=False)
                else:
                    results.to_csv(
                        f'{data_name}_many_train_new.csv', index=False)


'''Get average coverage and width for different alpha (5 values between 0.75 and 0.95)
    Note, ARIMA is used here and is compared against EnbPI
    Regression model is Ridge, RF, and NN'''
alpha_ls = np.linspace(0.05, 0.25, 5)
tot_trial = 10  # For CP
methods = ['Ensemble']
for one_dim in [True, False]:
    for data_name in Data_name:
        data = eval(f'data{data_ind[data_name]}')  # Pandas DataFrame
        data_x = data.loc[:, data.columns != response_ls[data_name]]
        data_y = data[response_ls[data_name]]
        data_x_numpy = data_x.to_numpy()  # Convert to numpy
        data_y_numpy = data_y.to_numpy()  # Convert to numpy
        total_data_points = data_x_numpy.shape[0]
        train_size = int(0.2 * total_data_points)
        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',
                                        'method', 'alpha', 'coverage', 'width'])
        results_ts = pd.DataFrame(columns=['itrial', 'dataname',
                                           'method', 'alpha', 'coverage', 'width'])
        for itrial in range(tot_trial):
            np.random.seed(98765 + itrial)
            for alpha in alpha_ls:
                nnet = keras_mod()  # Note, this is necessary because a model may "remember the past"
                rnnet = keras_rnn()
                print(f'At trial # {itrial} and alpha={alpha}')
                print(f'For {data_name}')
                if energy_data:
                    X_train, X_predict, Y_train, Y_predict = big_transform(
                        Data_name, data_name, one_dim, train_size)
                    d = 20
                else:
                    X_train = data_x_numpy[:train_size, :]
                    X_predict = data_x_numpy[train_size:, :]
                    Y_train = data_y_numpy[:train_size]
                    Y_predict = data_y_numpy[train_size:]
                    d = 20  # for 1-d memory depth
                    if one_dim:
                        X_train, X_predict, Y_train, Y_predict = one_dimen_transform(
                            Y_train, Y_predict, d=d)
                ridge_results = prediction_interval(
                    ridge_cv,  X_train, X_predict, Y_train, Y_predict)
                rf_results = prediction_interval(
                    random_forest,  X_train, X_predict, Y_train, Y_predict)
                nn_results = prediction_interval(
                    nnet,  X_train, X_predict, Y_train, Y_predict)
                if rnn:
                    T, k = X_train.shape
                    T1 = X_predict.shape[0]
                    X_train = X_train.reshape((T, 1, k))
                    X_predict = X_predict.reshape((T1, 1, k))
                    rnn_results = prediction_interval(
                        rnnet, X_train, X_predict, Y_train, Y_predict)
                if itrial == 0:
                    # For ARIMA, only run once
                    result_ts = ridge_results.run_experiments(
                        alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods, none_CP=True)
                    result_ts.rename(columns={'train_size': 'alpha'}, inplace=True)
                    if one_dim:
                        result_ts['alpha'].replace(
                            train_size - d, alpha, inplace=True)
                    else:
                        result_ts['alpha'].replace(train_size, alpha, inplace=True)
                    results_ts = pd.concat([results_ts, result_ts])
                    results_ts.to_csv(f'{data_name}_many_alpha_new_ARIMA.csv', index=False)
                # CP Methods
                print(f'regressor is {ridge_cv.__class__.__name__}')
                result_ridge = ridge_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                print(f'regressor is {random_forest.__class__.__name__}')
                result_rf = rf_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                print(f'regressor is {nnet.name}')
                # start = time.time()
                result_nn = nn_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                if rnn:
                    print(f'regressor is {rnnet.name}')
                    result_rnn = rnn_results.run_experiments(
                        alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods)
                    result_rnn['muh_fun'] = 'RNN'
                    results_now = pd.concat([result_ridge, result_rf, result_nn, result_rnn])
                else:
                    results_now = pd.concat([result_ridge, result_rf, result_nn])
                results_now.rename(columns={'train_size': 'alpha'}, inplace=True)
                if one_dim:
                    results_now['alpha'].replace(
                        train_size - d, alpha, inplace=True)
                else:
                    results_now['alpha'].replace(train_size, alpha, inplace=True)
                results = pd.concat([results, results_now])
                if one_dim:
                    results.to_csv(f'{data_name}_many_alpha_new_1d.csv', index=False)
                else:
                    results.to_csv(f'{data_name}_many_alpha_new.csv', index=False)


def merge_arima(data_name, which):
    data1 = pd.read_csv(f'{data_name}_many_alpha_new{which}.csv')
    data2 = pd.read_csv(f'{data_name}_many_alpha_new_ARIMA.csv')
    data1 = pd.concat((data1, data2))
    data1.reset_index(inplace=True)
    print(data1.shape)
    data1.to_csv(f'{data_name}_many_alpha_new{which}.csv', index=False)


for data_name in Data_name:
    merge_arima(data_name, '_1d')
    merge_arima(data_name, '')


''' Plots'''
Data_name = ['green_house', 'appliances', 'Beijing_air', 'Solar_Atl', 'Palo_Alto']
"""Mean Coverage over target coverage
    (ideally a straight line by diagonal)"""
alpha_ls = np.linspace(0.05, 0.25, 5)
x_axis = 1-alpha_ls
x_axis_name = 'alpha'
for dataname in Data_name:
    for two_rows in [False]:
        alg_slide_k.plot_average_new(x_axis, x_axis_name, Dataname=[dataname], two_rows=two_rows)

"""Grouped boxplot, alpha=0.1, over both univariate and multivariate data for
    certain fractions of training data."""
for data_name in Data_name:
    alg_slide_k.grouped_box_new(data_name, 'coverage')
    alg_slide_k.grouped_box_new(data_name, 'width')


'''Tables: Get averahe coverage, width, and Winkler score over 10 trials into a table'''


def Latex_table_by_regr(array_ls, regr_name, name_ls=[]):
    # For each data_name, two rows (first row LOO, second Ensemble)
    if len(name_ls) > 0:
        names = []
        for name in name_ls:
            names.append([f'{name}',  '', '', ''])
        names = np.hstack(names)
    else:
        return 0
    method = ['ARIMA', 'Ensemble', 'ICP', 'WeightedICP']
    array_t = np.zeros(len(array_ls), dtype=(
        '<U50,<U30, float64, float64, float64'))
    for j in range(len(names)):
        remainder = np.mod(j, 4)
        if remainder == 0:
            if '1d' in regr_name:
                array_t[j] = ' ', ' ', array_ls[j, 0], array_ls[j, 1], np.round(array_ls[j, 2], 3)
            else:
                name = '\multirow{4}{*}{' + names[j] + '}'
                array_t[j] = name, method[remainder], array_ls[j,
                                                               0], array_ls[j, 1], np.round(array_ls[j, 2], 3)
        else:
            if '1d' in regr_name:
                array_t[j] = ' ', ' ', array_ls[j, 0], array_ls[j, 1], np.round(array_ls[j, 2], 3)
            else:
                array_t[j] = ' ', method[remainder], array_ls[j,
                                                              0], array_ls[j, 1], np.round(array_ls[j, 2], 3)
    if len(name_ls) > 0:
        np.savetxt(f"cov_wid_score_{regr_name}_solar.txt", array_t, fmt=(
            '%s', '%s', '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\n', comments='')
    else:
        np.savetxt(f"cov_wid_score_{regr_name}.txt", array_t, fmt=(
            '%s', '%s', '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\n', comments='')


# Get average coverage & widths under X% training data as a table
tot_trial = 10
regr_names = {'RidgeCV': 'ridge'}
data_ind = {}
k = 0
for city in CA_cities:
    data_ind[city] = city
    k += 1
rnn = False
Data_name = CA_cities
for one_dim in [True, False]:
    regr_methods_name = ['RidgeCV']
    # For a particular regressor
    methods_name = ['Ensemble', 'ICP', 'Weighted_ICP', 'ARIMA']
    row_len = len(Data_name) * len(methods_name)
    col_len = 3  # coverage, width, Winkler score
    ridge_table_result = np.zeros((row_len, col_len, tot_trial))
    methods = ['Ensemble', 'ICP', 'Weighted_ICP']
    if rnn:
        rnn_table_result = np.zeros((row_len, col_len, tot_trial))
    for itrial in range(tot_trial):
        k = 0
        for data_name in Data_name:
            np.random.seed(98765 + itrial)
            nnet = keras_mod()
            rnnet = keras_rnn()
            print(f'Trial # {itrial} for data {data_name}')
            data = eval(f'data{data_ind[data_name]}')  # Pandas DataFrame
            data_x = data.loc[:, data.columns != response_ls[data_name]]
            data_y = data[response_ls[data_name]]
            data_x_numpy = data_x.to_numpy()  # Convert to numpy
            data_y_numpy = data_y.to_numpy()  # Convert to numpy
            total_data_points = data_x_numpy.shape[0]
            train_size = int(0.2 * total_data_points)
            results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',
                                            'method', 'train_size', 'coverage', 'width'])
            if energy_data:
                X_train, X_predict, Y_train, Y_predict = big_transform(
                    Data_name, data_name, one_dim, train_size)
            else:
                X_train = data_x_numpy[:train_size, :]
                X_predict = data_x_numpy[train_size:, :]
                Y_train = data_y_numpy[:train_size]
                Y_predict = data_y_numpy[train_size:]
                if one_dim:
                    X_train, X_predict, Y_train, Y_predict = one_dimen_transform(
                        Y_train, Y_predict, d=20)
            if itrial < 1:
                arima_results = prediction_interval(
                    ridge_cv, X_train, X_predict, Y_train, Y_predict)
            ridge_results = prediction_interval(
                ridge_cv, X_train, X_predict, Y_train, Y_predict)
            if itrial < 1:
                PI_cov_wid_arima = arima_results.run_experiments(
                    alpha, B, stride, data_name, itrial, miss_test_idx, get_plots=True, none_CP=True, methods=methods)
            print(f'regressor is Ridge')
            PI_cov_wid_ridge = ridge_results.run_experiments(
                alpha, B, stride, data_name, itrial, miss_test_idx, get_plots=True, methods=methods)
            if itrial < 1:
                arima_scores = arima_results.Winkler_score(
                    PI_cov_wid_arima[:-1][0], Data_name[k], methods, alpha, none_CP=True)[0]
            ridge_scores = ridge_results.Winkler_score(
                PI_cov_wid_ridge[:-1], Data_name[k], methods, alpha)
            # Store results for a particular regressor
            for regr_method in regr_methods_name:
                # Store results
                # Note, *4 because each dataset is coupled with 4 PI methods: ARIMA,EnPI, ICP, and WeightedICP
                regr_name = regr_names[regr_method]
                if itrial < 1:
                    eval(f'{regr_name}_table_result')[k * 4,
                                                      0, :] = PI_cov_wid_arima[-1]['coverage']
                eval(f'{regr_name}_table_result')[1 + k * 4:(k + 1)
                                                  * 4, 0, itrial] = eval(f'PI_cov_wid_{regr_name}')[-1]['coverage']
                eval(f'{regr_name}_table_result')[
                    k * 4, 1, itrial] = PI_cov_wid_arima[-1]['width']
                eval(f'{regr_name}_table_result')[1 + k * 4:(k + 1)
                                                  * 4, 1, itrial] = eval(f'PI_cov_wid_{regr_name}')[-1]['width']
                eval(f'{regr_name}_table_result')[
                    k * 4, 2, itrial] = arima_scores
                eval(f'{regr_name}_table_result')[
                    1 + k * 4:(k + 1) * 4, 2, itrial] = eval(f'{regr_name}_scores')
            print(eval(f'{regr_name}_table_result')[:, :, itrial])
            k += 1
        # Quick check
        np.set_printoptions(precision=2)
        ridge_table_result_save = np.mean(ridge_table_result, axis=2)
        if one_dim:
            # 1d
            Latex_table_by_regr(ridge_table_result_save, 'ridge_1d', name_ls=Data_name)
        else:
            # Non-1d
            Latex_table_by_regr(ridge_table_result_save, 'ridge', name_ls=Data_name)
'''




















'''
'''   Real-Data Experiments   '''
''' Stride s > 1: Multi-step ahead inference'''

'''Functions'''


def missing_data(data, missing_frac, update=False):
    n = len(data)
    idx = np.random.choice(n, size=int(missing_frac*n), replace=False)
    if update:
        data = np.delete(data, idx, 0)
    idx = idx.tolist()
    return (data, idx)


def keras_mod():
    model = Sequential(name='NeuralNet')
    model.add(Dense(100, activation='relu'))
    model.add(Dense(100, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(100, activation='relu'))
    model.add(Dense(1, activation='relu'))
    # Compile model
    opt = Adam(0.0005)
    model.compile(loss='mean_squared_error', optimizer=opt)
    return model


'''Main Helpers'''


def restructure_X_t(darray):
    '''
    For each row i after the first row, take i-1 last entries of the first row and then impute the rest
    Imputation is just generating random N(Y_train_mean, Y_train_std), where
    Y_train is the first row.
    '''
    s = darray.shape[1]
    copy = np.copy(darray)
    for i in range(1, min(s, darray.shape[0])):
        copy[i, :s-i] = copy[0, i:]
        imputed_val = np.abs(np.random.normal(loc=np.mean(
            copy[0]), scale=np.std(copy[0]), size=i))
        copy[i, s-i:] = imputed_val
    return copy


def further_preprocess(data, response_name='DHI'):
    '''Extract non-zero hours and also hours between 10AM-2PM (where radiation is high) '''
    max_recorder = pd.DataFrame(np.zeros(24), index=range(0, 24))
    for i in range(0, 24):
        # Check at what times max recording is 0 (meaning no recording yet)
        time = np.arange(365)*24+i  # 12:00 AM every day. for every later hour, + i \in \{1,...,23\}
        max_record = np.max(data[response_name][time])
        max_recorder.iloc[i] = max_record
    # Drop these non-zero things
    data_sub = data.copy()
    to_be_droped = np.where(max_recorder == 0)[0]
    print(to_be_droped)
    drop_idx = []
    if len(to_be_droped) > 0:
        for i in to_be_droped:
            drop_idx.append(np.arange(365)*24+i)
        drop_idx = np.hstack(drop_idx)
        data_sub.drop(drop_idx, inplace=True)
    else:
        data_sub = []
    # Create near_noon data between 10AM-2PM
    to_be_included = np.array([10, 11, 12, 13, 14])
    to_be_droped = np.delete(np.arange(24), to_be_included)
    data_near_noon = data.copy()
    drop_idx = []
    for i in to_be_droped:
        drop_idx.append(np.arange(365)*24+i)
    drop_idx = np.hstack(drop_idx)
    data_near_noon.drop(drop_idx, inplace=True)
    return [data_sub, data_near_noon]


def big_transform_s_beyond_1(sub, cities, current_city, one_dim, missing, miss_frac=0.25):
    '''Overall, include ALL other cities' data in the CURRENT city being considered.
       1. Check what data is used (full, sub, or near-noon), need sub, but it is now suppressed.
       # NOTE, 1 is suppressed for now, since we are uncertain whether sub or near-noon is needed for Californian results
       2. If missing, process these training and testing data before transform
       -->> Current city and neighbors are assumed to have DIFFERENT missing fractions.
       3. Then, if one_dim, transform data (include past), but since s>1, apply *restructure_X_t* to s rows a time'''
    big_X_train = []
    big_X_predict = []
    for city in cities:
        print(city)
        # Start 1
        data_full = eval(f'data{city}')  # Pandas DataFrame
        if city == 'Wind_Austin':
            data_sub, data_near_noon = further_preprocess(data_full, response_name='MWH')
        else:
            data_sub, data_near_noon = further_preprocess(data_full)
        if sub == 0:
            data = data_full
            stride = 24
        elif sub == 1:
            data = data_sub
            stride = int(len(data)/365)
        else:
            data = data_near_noon
            stride = 5
        train_size = 92*stride
        col_name = 'MWH' if city == 'Wind_Austin' else 'DHI'
        data_x = data.loc[:, data.columns != col_name]
        data_y = data[col_name]
        data_x_numpy = data_x.to_numpy()  # Convert to numpy
        data_y_numpy = data_y.to_numpy()  # Convert to numpy
        X_train = data_x_numpy[:train_size, :]
        X_predict = data_x_numpy[train_size:, :]
        Y_train_del = data_y_numpy[:train_size]
        Y_predict_del = data_y_numpy[train_size:]
        # Finish 1
        # Start 2
        if missing:
            X_train, miss_train_idx = missing_data(
                X_train, missing_frac=miss_frac, update=True)
            Y_train_del = np.delete(Y_train_del, miss_train_idx)
            Y_predict_del, miss_test_idx = missing_data(
                Y_predict_del, missing_frac=miss_frac, update=False)
            if city == current_city:
                # Need an additional Y_truth
                Y_train = Y_train_del
                Y_predict = Y_predict_del.copy()
                true_miss_text_idx = miss_test_idx
            Y_predict_del[miss_test_idx] = np.abs(np.random.normal(loc=np.mean(
                Y_train_del), scale=np.std(Y_train_del), size=len(miss_test_idx)))

        else:
            true_miss_text_idx = []
            if city == current_city:
                Y_train = Y_train_del
                Y_predict = Y_predict_del
        # Finish 2
        # Start 3
        if one_dim:
            X_train, X_predict, Y_train_del, Y_predict_del = one_dimen_transform(
                Y_train_del, Y_predict_del, d=min(stride, 24))  # Note: this handles 'no_slide (stride=infty)' case
            j = 0
            for k in range(len(X_predict)//stride+1):
                X_predict[j*k:min((j+1)*k, len(X_predict))
                          ] = restructure_X_t(X_predict[j*k:min((j+1)*k, len(X_predict))])
                j += 1
            big_X_train.append(X_train)
            big_X_predict.append(X_predict)
            if city == current_city:
                Y_train = Y_train_del
        else:
            big_X_train.append(X_train)
            big_X_predict.append(X_predict)
        # Finish 3
    X_train = np.abs(np.hstack(big_X_train))
    X_predict = np.abs(np.hstack(big_X_predict))
    return([X_train, X_predict, Y_train, Y_predict, true_miss_text_idx, stride])


def all_together(Data_name, sub, no_slide, missing, miss_frac=0.25, one_dim=False):
    methods = ['Ensemble']
    train_days = 92
    density_est = False
    itrial = 1
    results_ls = {}
    alpha = 0.1
    B = np.random.binomial(100, np.exp(-1))  # number of bootstrap samples
    for data_name in Data_name:
        np.random.seed(98765)
        # nnet = keras_mod()  # Note, this is necessary because a model may "remember the past"
        X_train, X_predict, Y_train, Y_predict, miss_test_idx, stride = big_transform_s_beyond_1(
            sub, Data_name, data_name, one_dim, missing)
        train_size = 92*stride
        print(f'At train_size={train_size}')
        print(f'For {data_name}')
        if no_slide:
            stride = int((365-92)*stride)  # No slide at all
        print(stride)
        nnet = keras_mod()
        min_alpha = 0.0001
        max_alpha = 10
        ridge_cv = RidgeCV(alphas=np.linspace(min_alpha, max_alpha, 10))
        random_forest = RandomForestRegressor(n_estimators=10, criterion='mse',
                                              bootstrap=False, max_depth=2, n_jobs=-1)
        ridge_results = prediction_interval(
            ridge_cv, X_train, X_predict, Y_train, Y_predict)
        rf_results = prediction_interval(
            random_forest,  X_train, X_predict, Y_train, Y_predict)
        nn_results = prediction_interval(
            nnet,  X_train, X_predict, Y_train, Y_predict)
        # For CP Methods
        print(f'regressor is {ridge_cv.__class__.__name__}')
        result_ridge = ridge_results.run_experiments(
            alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods, get_plots=True)
        print(f'regressor is {random_forest.__class__.__name__}')
        result_rf = rf_results.run_experiments(
            alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods, get_plots=True)
        print(f'regressor is {nnet.name}')
        result_nn = nn_results.run_experiments(
            alpha, B, stride, data_name, itrial, miss_test_idx, methods=methods, get_plots=True)
        results_ls[data_name] = [result_ridge, result_rf, result_nn, stride, Y_predict]
    return results_ls


'''Plotting functions'''


def PI_on_series_plus_cov_or_not(results, stride, which_hours, which_method, regr_method, Y_predict, no_slide=False, five_in_a_row=True):
    # Plot PIs on predictions for the particular hour
    # At most three plots in a row (so that figures look appropriately large)
    plt.rcParams.update({'font.size': 18})
    if five_in_a_row:
        ncol = 5
    else:
        ncol = 3
    nrow = np.ceil(len(which_hours)/ncol).astype(int)
    if stride == 24 or stride == 14 or stride == 15:
        # Multi-row
        fig, ax = plt.subplots(nrow*2, ncol, figsize=(ncol*4, nrow*5), sharex='row',
                               sharey='row', constrained_layout=True)
    else:
        fig, ax = plt.subplots(2, 5, figsize=(5*4, 5), sharex='row',
                               sharey='row', constrained_layout=True)
    if stride > 24:
        n1 = int(results[0].shape[0]/5)  # Because we focused on near-noon-data
    else:
        n1 = int(results[0].shape[0]/stride)
    plot_length = 91  # Plot 3 months, April-June
    method_ls = {'Ensemble': 0, 'ICP': 1, 'WeightedICP': 2}
    results_by_method = results[method_ls[which_method]]
    for i in range(len(which_hours)):
        hour = which_hours[i]
        if stride > 24:
            indices_at_hour = np.arange(n1)*5+hour
        else:
            indices_at_hour = np.arange(n1)*stride+hour
        to_plot = indices_at_hour[:plot_length]
        row = (i//ncol)*2
        col = np.mod(i, ncol)
        covered_or_not = []
        for j in range(n1):
            if Y_predict[indices_at_hour[j]] >= results_by_method['lower'][indices_at_hour[j]] and Y_predict[indices_at_hour[j]] <= results_by_method['upper'][indices_at_hour[j]]:
                covered_or_not.append(1)
            else:
                covered_or_not.append(0)
        coverage = np.mean(covered_or_not)
        coverage = np.round(coverage, 2)
        # Plot PI on data
        train_size = 92
        rot_angle = 15
        x_axis = np.arange(plot_length)
        if stride == 24 or stride == 14 or stride == 15:
            current_figure = ax[row, col]
        else:
            col = np.mod(i, 5)
            current_figure = ax[0, col]
        current_figure.scatter(x_axis, Y_predict[to_plot], marker='.', s=3, color='black')
        current_figure.plot(x_axis, np.maximum(0, results_by_method['upper'][to_plot]))
        current_figure.plot(x_axis, np.maximum(0, results_by_method['lower'][to_plot]))
        xticks = np.linspace(0, plot_length-30, 3).astype(int)  # For axis purpose, subtract June
        xtick_labels = [calendar.month_name[int(i/30)+4]
                        for i in xticks]  # Get months, start from April
        current_figure.set_xticks(xticks)
        current_figure.set_xticklabels(xtick_labels)
        current_figure.tick_params(axis='x', rotation=rot_angle)
        # Title
        if stride == 24:
            current_figure.set_title(f'At {hour}:00 \n Coverage is {coverage}')
        elif stride == 5 or no_slide:
            current_figure.set_title(f'At {hour+10}:00 \n Coverage is {coverage}')
        else:
            if stride == 15:
                current_figure.set_title(f'At {hour+5}:00 \n Coverage is {coverage}')
            else:
                current_figure.set_title(f'At {hour+6}:00 \n Coverage is {coverage}')
        # if stride == 14:
        #     # Sub data`
        #     current_figure.set_title(f'At {hour+6}:00 \n Coverage is {coverage}')
        # elif stride == 24:
        #     # Full data
        #     current_figure.set_title(f'At {hour}:00 \n Coverage is {coverage}')
        # else:
        #     # Near noon data
        #     current_figure.set_title(f'At {hour+10}:00 \n Coverage is {coverage}')
        # Plot cover or not over test period
        x_axis = np.arange(n1)
        if stride == 24 or stride == 14 or stride == 15:
            current_figure = ax[row+1, col]
        else:
            col = np.mod(i, 5)
            current_figure = ax[1, col]
        current_figure.scatter(x_axis, covered_or_not, marker='.', s=0.4)
        current_figure.set_ylim([-1, 2])
        xticks = np.linspace(0, n1-31, 3).astype(int)  # For axis purpose, subtract December
        xtick_labels = [calendar.month_name[int(i/30)+4] for i in xticks]  # Get months
        current_figure.set_xticks(xticks)
        current_figure.set_xticklabels(xtick_labels)
        current_figure.tick_params(axis='x', rotation=rot_angle)
        yticks = [0, 1]
        current_figure.set_yticks(yticks)
        current_figure.set_yticklabels(['Uncovered', 'Covered'])
        # xticks = current_figure.get_xticks()  # Actual numbers
        # xtick_labels = [f'T+{int(i)}' for i in xticks]
        # current_figure.set_xticklabels(xtick_labels)
    # if no_slide:
    #     fig.suptitle(
    #         f'EnbPI Intervals under {regr_method} without sliding', fontsize=22)
    # else:
    #     fig.suptitle(
    #         f'EnbPI Intervals under {regr_method} with s={stride}', fontsize=22)
    return fig


def make_cond_plots(Data_name, results_ls, no_slide, missing, one_d, five_in_a_row=True):
    for data_name in Data_name:
        result_ridge, result_rf, result_nn, stride, Y_predict = results_ls[data_name]
        res = [result_ridge, result_rf, result_nn]
        if no_slide:
            which_hours = [0, 1, 2, 3, 4]  # 10AM-2PM
        else:
            if stride == 24:
                if five_in_a_row:
                    which_hours = [7, 8, 9, 16, 17, 10, 11, 12, 13, 14]
                else:
                    which_hours = [7, 8, 10, 11, 12, 13, 14, 16, 17]
            elif stride == 5:
                which_hours = [0, 1, 2, 3, 4]
            else:
                if five_in_a_row:
                    if data_name == 'Solar_Atl':
                        which_hours = [i-6 for i in [7, 8, 9, 16, 17, 10, 11, 12, 13, 14]]
                    else:
                        which_hours = [i-5 for i in [7, 8, 9, 16, 17, 10, 11, 12, 13, 14]]
                else:
                    if data_name == 'Solar_Atl':
                        # which_hours = [i-6 for i in [7, 8, 10, 11, 12, 13, 14, 16, 17]]
                        which_hours = [i-6 for i in [8, 9, 16, 11, 12, 13]]
                    else:
                        # which_hours = [i-5 for i in [7, 8, 10, 11, 12, 13, 14, 16, 17]]
                        which_hours = [i-5 for i in [8, 9, 16, 11, 12, 13]]
        which_method = 'Ensemble'
        regr_methods = {0: 'Ridge', 1: 'RF', 2: 'NN'}
        X_data_type = {True: 'uni', False: 'multi'}
        Xtype = X_data_type[one_d]
        slide = '_no_slide' if no_slide else '_daily_slide'
        Dtype = {24: '_fulldata', 14: '_subdata', 15: '_subdata', 5: '_near_noon_data'}
        if no_slide:
            dtype = ''
        else:
            dtype = Dtype[stride]
        miss = '_with_missing' if missing else ''
        for i in range(len(res)):
            regr_method = regr_methods[i]
            fig = PI_on_series_plus_cov_or_not(
                res[i], stride, which_hours, which_method, regr_method, Y_predict, no_slide, five_in_a_row)
            fig.savefig(f'{data_name}_{regr_method}_{Xtype}_PI_on_series_plus_cov_or_not{slide}{dtype}{miss}.pdf', dpi=300, bbox_inches='tight',
                        pad_inches=0)


''' Solar ATL '''
ATL_cities = ['Solar_Atl']
max_data_size = 10000
dataSolar_Atl = read_data(3, '../Real Data/Solar_Atl_data.csv', max_data_size)

'''1. Multi-step inference: first three are for my EnbPI, last two for no slide'''
# s=14
results_ls_one_d_no_missing_and_slide_sub = all_together(
    Data_name=ATL_cities, sub=1, no_slide=False, missing=False, one_dim=True)
results_ls_no_missing_and_slide_sub = all_together(
    Data_name=ATL_cities, sub=1, no_slide=False, missing=False, one_dim=False)

# s=5
results_ls_one_d_no_missing_and_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=False, missing=False, one_dim=True)
results_ls_no_missing_and_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=False, missing=False, one_dim=False)

# No slide: can have poor coverage even if near-noon data is used
results_ls_one_d_no_missing_and_no_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=True, missing=False, one_dim=True)
results_ls_no_missing_and_no_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=True, missing=False, one_dim=False)

'''2. Multi-step inference under missing data using EnbPI'''

results_ls_one_d_with_missing_and_slide_sub = all_together(
    Data_name=ATL_cities, sub=1, no_slide=False, missing=True, one_dim=True)
results_ls_with_missing_and_slide_sub = all_together(
    Data_name=ATL_cities, sub=1, no_slide=False, missing=True, one_dim=False)
results_ls_one_d_with_missing_and_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=False, missing=True, one_dim=True)
results_ls_with_missing_and_slide_near_noon = all_together(
    Data_name=ATL_cities, sub=2, no_slide=False, missing=True, one_dim=False)
'''Plots'''

Data_name = ATL_cities

# s=14
make_cond_plots(Data_name, results_ls_no_missing_and_slide_sub,
                no_slide=False, missing=False, one_d=False, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_one_d_no_missing_and_slide_sub,
                no_slide=False, missing=False, one_d=True, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_with_missing_and_slide_sub,
                no_slide=False, missing=True, one_d=False, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_one_d_with_missing_and_slide_sub,
                no_slide=False, missing=True, one_d=True, five_in_a_row=False)

# s=5

make_cond_plots(Data_name, results_ls_one_d_no_missing_and_slide_near_noon,
                no_slide=False, missing=False, one_d=True)
make_cond_plots(Data_name, results_ls_no_missing_and_slide_near_noon,
                no_slide=False, missing=False, one_d=False)
make_cond_plots(Data_name, results_ls_one_d_no_missing_and_no_slide_near_noon,
                no_slide=True, missing=False, one_d=True)
make_cond_plots(Data_name, results_ls_no_missing_and_no_slide_near_noon,
                no_slide=True, missing=False, one_d=False)
make_cond_plots(Data_name, results_ls_one_d_with_missing_and_slide_near_noon,
                no_slide=False, missing=True, one_d=True)
make_cond_plots(Data_name, results_ls_with_missing_and_slide_near_noon,
                no_slide=False, missing=True, one_d=False)
'''




'''

''' Solar CA '''
CA_cities = ['Fremont', 'Milpitas', 'Mountain_View', 'North_San_Jose',
             'Palo_Alto', 'Redwood_City', 'San_Mateo', 'Santa_Clara',
             'Sunnyvale']
CA_cities = ['Palo_Alto']
for city in CA_cities:
    globals()['data%s' % city] = read_CA_data(f'../Real Data/{city}_data.csv')

'''1. Multi-step inference: first two are for my EnbPI, last two for no slide'''
# Returns a dictionary with results at each city in CA
# s=15 (not 14)
results_ls_one_d_no_missing_and_slide_sub = all_together(
    Data_name=CA_cities, sub=1, no_slide=False, missing=False, one_dim=True)
results_ls_no_missing_and_slide_sub = all_together(
    Data_name=CA_cities, sub=1, no_slide=False, missing=False, one_dim=False)


# s=5
results_ls_one_d_no_missing_and_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=False, missing=False, one_dim=True)
results_ls_no_missing_and_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=False, missing=False, one_dim=False)

# No_slide: can have poor coverage even if near-noon data is used
results_ls_one_d_no_missing_and_no_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=True, missing=False, one_dim=True)

results_ls_no_missing_and_no_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=True, missing=False, one_dim=False)

'''2. Multi-step inference under missing data using EnbPI'''
# s=15 (not 14)
results_ls_one_d_with_missing_and_slide_sub = all_together(
    Data_name=CA_cities, sub=1, no_slide=False, missing=True, one_dim=True)
results_ls_with_missing_and_slide_sub = all_together(
    Data_name=CA_cities, sub=1, no_slide=False, missing=True, one_dim=False)

# s=5

results_ls_one_d_with_missing_and_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=False, missing=True, one_dim=True)

results_ls_with_missing_and_slide_near_noon = all_together(
    Data_name=CA_cities, sub=2, no_slide=False, missing=True, one_dim=False)

'''Plots'''

Data_name = CA_cities
# s=14
make_cond_plots(Data_name, results_ls_no_missing_and_slide_sub,
                no_slide=False, missing=False, one_d=False, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_one_d_no_missing_and_slide_sub,
                no_slide=False, missing=False, one_d=True, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_with_missing_and_slide_sub,
                no_slide=False, missing=True, one_d=False, five_in_a_row=False)
make_cond_plots(Data_name, results_ls_one_d_with_missing_and_slide_sub,
                no_slide=False, missing=True, one_d=True, five_in_a_row=False)

# s=5

make_cond_plots(Data_name, results_ls_one_d_no_missing_and_slide_near_noon,
                no_slide=False, missing=False, one_d=True)
make_cond_plots(Data_name, results_ls_no_missing_and_slide_near_noon,
                no_slide=False, missing=False, one_d=False)
make_cond_plots(Data_name, results_ls_one_d_no_missing_and_no_slide_near_noon,
                no_slide=True, missing=False, one_d=True)
make_cond_plots(Data_name, results_ls_no_missing_and_no_slide_near_noon,
                no_slide=True, missing=False, one_d=False)
make_cond_plots(Data_name, results_ls_one_d_with_missing_and_slide_near_noon,
                no_slide=False, missing=True, one_d=True)
make_cond_plots(Data_name, results_ls_with_missing_and_slide_near_noon,
                no_slide=False, missing=True, one_d=False)


'''


'''

'''Wind
   From https://github.com/Duvey314/austin-green-energy-predictor'''
Wind_cities = ['Wind_Austin']
dataWind_Austin = read_wind_data()
# Note, no missing entries, so only sub=0 used.
results_ls_one_d_no_missing_and_slide_full = all_together(
    Data_name=Wind_cities, sub=0, no_slide=False, missing=False, one_dim=True)

results_ls_one_d_with_missing_and_slide_full = all_together(
    Data_name=Wind_cities, sub=0, no_slide=False, missing=True, one_dim=True)

'''Plots'''

Data_name = Wind_cities

make_cond_plots(Data_name, results_ls_one_d_no_missing_and_slide_full,
                no_slide=False, missing=False, one_d=True)
make_cond_plots(Data_name, results_ls_one_d_with_missing_and_slide_full,
                no_slide=False, missing=True, one_d=True)
